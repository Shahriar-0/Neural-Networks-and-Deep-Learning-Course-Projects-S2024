{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimised Neural Network\n",
    "\n",
    "In this notebook, we will look into common problem in neural networks, which is overfitting. We will use the a sin(x) and popular MNIST dataset for this purpose, and use different architectures and different train test splits to see how our model performs. Then we try to find optimal hyperparameters for our model to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Overfitting is a common problem in neural networks, where the model performs well on the training data but poorly on the test data. This is because the model has learned the noise in the training data, rather than the underlying pattern. Common reasons for overfitting include using a model that is too complex, not having enough training data, and training for too many epochs. There are several ways to reduce overfitting, such as using dropout, early stopping, and data augmentation. In this notebook, we will explore some of these techniques and see how they can be used to improve the performance of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imageio.v3 as iio\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pygifsicle import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_layers = glob(\"../data/OptimisedNN/sin/layers/*\")\n",
    "files_ratios = glob(\"../data/OptimisedNN/sin/ratios/*\")\n",
    "\n",
    "for file in files_layers:\n",
    "    shutil.rmtree(file)\n",
    "for file in files_ratios:\n",
    "    shutil.rmtree(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "At first we will use a simple sin(x) dataset to demonstrate overfitting. We will generate 1000 data points between 0 and 4*pi and use this as our training data. We will then use different architectures and train test splits to see how our model performs. We will then use the popular MNIST dataset to further demonstrate overfitting and explore different techniques to reduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression \n",
    "\n",
    "\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "# MATIN the analysis and report is not complete.\n",
    "\n",
    "We first analyze overfitting on a simple regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sin_values = np.linspace(0, 4 * np.pi, 1000)\n",
    "y_sin_values = np.sin(x_sin_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we try with 2-layer neural network with different ratios of train test splits, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIOS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "EPOCHS = [100, 200, 500]\n",
    "LOSS_FUNCTIONS = [\"mse\", \"mae\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are things like learning rate, batch size, number of epochs, etc. that are set before the training process begins. These are the parameters that are set before the training process begins, and are not learned by the model. Hyperparameters can have a significant impact on the performance of a model, and finding the right hyperparameters can be a challenging task. In this notebook, we will explore some common hyperparameters and see how they can be tuned to improve the performance of a neural network.\n",
    "\n",
    "Some common hyperparameters include:\n",
    "\n",
    "- Learning rate: The rate at which the model learns from the training data. A higher learning rate can help the model converge faster, but may result in overshooting the optimal solution. A lower learning rate can help the model converge more slowly, but may result in getting stuck in local minima.\n",
    "- Batch size: The number of samples that are used to update the model parameters in each iteration. A larger batch size can help the model converge faster, but may require more memory. A smaller batch size can help the model converge more slowly, but may result in more noise in the updates.\n",
    "- Number of epochs: The number of times the model sees the entire training data. A larger number of epochs can help the model learn more from the training data, but may result in overfitting. A smaller number of epochs can help the model generalize better, but may result in underfitting.\n",
    "- Activation function: The function that is applied to the output of each neuron. Common activation functions include `ReLU`, sigmoid, and tanh. The choice of activation function can have a significant impact on the performance of the model.\n",
    "- Number of layers: The number of layers in the neural network. A deeper network can learn more complex patterns, but may require more data and more training time. A shallower network may be easier to train, but may not be able to learn as complex patterns.\n",
    "- Number of neurons: The number of neurons in each layer of the neural network. A larger number of neurons can help the model learn more complex patterns, but may require more data and more training time. A smaller number of neurons may be easier to train, but may not be able to learn as complex patterns.\n",
    "- Dropout rate: The rate at which neurons are randomly dropped out during training. Dropout can help prevent overfitting by reducing the co-adaptation of neurons, but may require more training time.\n",
    "- Optimizer: The algorithm used to update the model parameters based on the loss function. Common optimizers include SGD, Adam, and RMSprop. The choice of optimizer can have a significant impact on the performance of the model.\n",
    "- Weight initialization: The technique used to initialize the weights of the neural network. Common weight initialization techniques include random initialization, Xavier initialization, and He initialization. The choice of weight initialization can have a significant impact on the performance of the model.\n",
    "- Learning rate schedule: The technique used to adjust the learning rate during training. Common learning rate schedules include step decay, exponential decay, and cosine annealing. The choice of learning rate schedule can have a significant impact on the performance of the model.\n",
    "- Regularization Parameter: The parameter used to control the strength of the regularization term. A larger regularization parameter can help prevent overfitting, but may result in underfitting. A smaller regularization parameter may not be able to prevent overfitting, but may result in better performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_layers, num_neurons, activation, loss):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(1,)))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(num_neurons, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_stats(mses, rmses, maes, y_true, y_pred, ratio=None, num_layer=None):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mses.append(mse)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    rmses.append(rmse)\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    maes.append(mae)\n",
    "    \n",
    "    print(\"For \" + (f\"ratio {ratio}\" if ratio else f\"layer {num_layer}\"))\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/OptimisedNN/sin/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sin_predictions_ratio(loss, epochs, activation, num_neurons):\n",
    "    accuracy_mses = []\n",
    "    accuracy_rmses = []\n",
    "    accuracy_maes = []\n",
    "    \n",
    "    model_path = (\n",
    "        FILE_PATH\n",
    "        + \"ratios/\"\n",
    "        + f\"model_layers.{2}_neurons.{num_neurons}_epochs.{epochs}_activation.{activation}_loss.{loss}/\"\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    for i, ratio in enumerate(TRAIN_TEST_RATIOS):\n",
    "        model = create_model(2, num_neurons, activation, loss)\n",
    "        file_path = model_path + f\"ratio.{ratio}/\"\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(file_path)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_sin_values, y_sin_values, test_size=ratio)\n",
    "        \n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=epochs, verbose=0)\n",
    "\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        error_stats(accuracy_mses, accuracy_rmses, accuracy_maes, y_test, y_test_pred, ratio=ratio)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.title(f\"Loss function changes for ratio {ratio}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        loss_filename = f\"{file_path}loss_{i}.png\"\n",
    "        plt.savefig(loss_filename)\n",
    "        plt.close()\n",
    "\n",
    "        y_pred = model.predict(x_sin_values)\n",
    "        plt.figure()\n",
    "        plt.plot(x_sin_values, y_sin_values, label=\"True function\")\n",
    "        plt.plot(x_sin_values, y_pred, label=\"Estimated function\")\n",
    "        plt.title(f\"Function estimation for ratio {ratio}\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        sin_filename = f\"{file_path}sin_{i}.png\"\n",
    "        plt.savefig(sin_filename)\n",
    "        plt.close()\n",
    "\n",
    "    frames_loss = np.stack(\n",
    "        [\n",
    "            iio.imread(f\"{model_path}ratio.{TRAIN_TEST_RATIOS[i]}/loss_{i}.png\")\n",
    "            for i in range(len(TRAIN_TEST_RATIOS))\n",
    "        ]\n",
    "    )\n",
    "    iio.imwrite(f\"{model_path}loss.gif\", frames_loss, fps=0.5)\n",
    "\n",
    "    frames_sin = np.stack(\n",
    "        [\n",
    "            iio.imread(f\"{model_path}ratio.{TRAIN_TEST_RATIOS[i]}/sin_{i}.png\")\n",
    "            for i in range(len(TRAIN_TEST_RATIOS))\n",
    "        ]\n",
    "    )\n",
    "    iio.imwrite(f\"{model_path}sin.gif\", frames_sin, fps=0.5)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(TRAIN_TEST_RATIOS, accuracy_mses, label=\"Mean Squared Error\")\n",
    "    plt.plot(TRAIN_TEST_RATIOS, accuracy_rmses, label=\"Root Mean Squared Error\")\n",
    "    plt.plot(TRAIN_TEST_RATIOS, accuracy_maes, label=\"Mean Absolute Error\")\n",
    "    plt.title(\"Error changes for different ratios\")\n",
    "    plt.xlabel(\"Train test ratio\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(f\"{model_path}errors.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # for removing files\n",
    "    # os.remove(loss_filename)\n",
    "    # os.remove(sin_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we try with MSE loss and 100 epochs and relu, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.9995403501126733\n",
      "Root Mean Squared Error: 0.999770148640513\n",
      "Mean Absolute Error: 0.8102009021073969\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.972737436119481\n",
      "Root Mean Squared Error: 0.9862745237100475\n",
      "Mean Absolute Error: 0.8016031009324875\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.8945123512560306\n",
      "Root Mean Squared Error: 0.9457866309353451\n",
      "Mean Absolute Error: 0.7718300980828083\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.8900276661332928\n",
      "Root Mean Squared Error: 0.9434127761130293\n",
      "Mean Absolute Error: 0.7702103805225946\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.8746587855352449\n",
      "Root Mean Squared Error: 0.9352319421059382\n",
      "Mean Absolute Error: 0.7658829283170827\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.6436160268363992\n",
      "Root Mean Squared Error: 0.8022568334619526\n",
      "Mean Absolute Error: 0.6821712586913147\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.6765271042104093\n",
      "Root Mean Squared Error: 0.8225126772338583\n",
      "Mean Absolute Error: 0.6968710356993896\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.6710412094746473\n",
      "Root Mean Squared Error: 0.8191710501932105\n",
      "Mean Absolute Error: 0.6978651978131986\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.7210691382364922\n",
      "Root Mean Squared Error: 0.8491578994724669\n",
      "Mean Absolute Error: 0.7075535115342898\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=100, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with 200 epochs and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.9047098313102786\n",
      "Root Mean Squared Error: 0.9511623580179561\n",
      "Mean Absolute Error: 0.7731208126032766\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.954609976750788\n",
      "Root Mean Squared Error: 0.9770414406517197\n",
      "Mean Absolute Error: 0.7928650315434146\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.934795193920955\n",
      "Root Mean Squared Error: 0.9668480717884042\n",
      "Mean Absolute Error: 0.7861933266747307\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.9215227015525179\n",
      "Root Mean Squared Error: 0.9599597395477156\n",
      "Mean Absolute Error: 0.7849118998087934\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9501596132661484\n",
      "Root Mean Squared Error: 0.9747613109198315\n",
      "Mean Absolute Error: 0.7928032028634839\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.8832078050490637\n",
      "Root Mean Squared Error: 0.9397913625103519\n",
      "Mean Absolute Error: 0.7662996547671551\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9220647511073392\n",
      "Root Mean Squared Error: 0.9602420273594252\n",
      "Mean Absolute Error: 0.783004767409014\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.699976141862703\n",
      "Root Mean Squared Error: 0.8366457684484533\n",
      "Mean Absolute Error: 0.704130388527229\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6249476917351755\n",
      "Root Mean Squared Error: 0.7905363316984081\n",
      "Mean Absolute Error: 0.6753842279022206\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=200, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with 500 epochs and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0165122171054486\n",
      "Root Mean Squared Error: 1.0082223053996815\n",
      "Mean Absolute Error: 0.8161786773536535\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9698468175735881\n",
      "Root Mean Squared Error: 0.9848080105145308\n",
      "Mean Absolute Error: 0.7994303946373708\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.9660836789912803\n",
      "Root Mean Squared Error: 0.9828955585367554\n",
      "Mean Absolute Error: 0.7979092972176925\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.9911368305404503\n",
      "Root Mean Squared Error: 0.9955585520402355\n",
      "Mean Absolute Error: 0.8071725506784944\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9728653941058228\n",
      "Root Mean Squared Error: 0.9863393909328689\n",
      "Mean Absolute Error: 0.8000092662807612\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.9876574154327172\n",
      "Root Mean Squared Error: 0.9938095468613276\n",
      "Mean Absolute Error: 0.8063401012854045\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9650112147824529\n",
      "Root Mean Squared Error: 0.982349843376815\n",
      "Mean Absolute Error: 0.7969197030553694\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.8810119728957357\n",
      "Root Mean Squared Error: 0.9386223803509779\n",
      "Mean Absolute Error: 0.767530219160055\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6950268201835023\n",
      "Root Mean Squared Error: 0.8336826855485858\n",
      "Mean Absolute Error: 0.7026099199198023\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=500, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with different loss functions and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0433200451532332\n",
      "Root Mean Squared Error: 1.021430391731729\n",
      "Mean Absolute Error: 0.8257449384771275\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.7687306490790797\n",
      "Root Mean Squared Error: 0.8767728605967909\n",
      "Mean Absolute Error: 0.7242903271224793\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.8937845310272563\n",
      "Root Mean Squared Error: 0.9454017828559751\n",
      "Mean Absolute Error: 0.7720363466287442\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.8593328407287582\n",
      "Root Mean Squared Error: 0.9270020715881697\n",
      "Mean Absolute Error: 0.7599701389918774\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.8464229811731323\n",
      "Root Mean Squared Error: 0.9200124896832284\n",
      "Mean Absolute Error: 0.754221982473058\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.7034980914096386\n",
      "Root Mean Squared Error: 0.8387479307930593\n",
      "Mean Absolute Error: 0.7005938146816986\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.8297799423174449\n",
      "Root Mean Squared Error: 0.91092257756488\n",
      "Mean Absolute Error: 0.7484423759730906\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.7131608163485874\n",
      "Root Mean Squared Error: 0.8444884939113069\n",
      "Mean Absolute Error: 0.7088178255166568\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6676027309123892\n",
      "Root Mean Squared Error: 0.8170695997969752\n",
      "Mean Absolute Error: 0.6937680417872978\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=100, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.12156299709413\n",
      "Root Mean Squared Error: 1.0590387136899813\n",
      "Mean Absolute Error: 0.8573444878739341\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.0081235080042557\n",
      "Root Mean Squared Error: 1.0040535384152858\n",
      "Mean Absolute Error: 0.8158504389704169\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 1.0523967003098587\n",
      "Root Mean Squared Error: 1.0258638800103348\n",
      "Mean Absolute Error: 0.8344142116255402\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0360835606016021\n",
      "Root Mean Squared Error: 1.0178818991423328\n",
      "Mean Absolute Error: 0.8259491629048465\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 1.0355952851619394\n",
      "Root Mean Squared Error: 1.0176420221089237\n",
      "Mean Absolute Error: 0.8266159713435496\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 1.0238948345741135\n",
      "Root Mean Squared Error: 1.0118768870638926\n",
      "Mean Absolute Error: 0.8227582444531757\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.7344715325473427\n",
      "Root Mean Squared Error: 0.8570131460761513\n",
      "Mean Absolute Error: 0.7174917064542773\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.7945399169864602\n",
      "Root Mean Squared Error: 0.8913696859252396\n",
      "Mean Absolute Error: 0.7347121923027936\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6455359974116424\n",
      "Root Mean Squared Error: 0.8034525483260616\n",
      "Mean Absolute Error: 0.6884492695869927\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=200, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0434785028138474\n",
      "Root Mean Squared Error: 1.0215079553355653\n",
      "Mean Absolute Error: 0.8230648700085909\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.0237649043831203\n",
      "Root Mean Squared Error: 1.0118126824581317\n",
      "Mean Absolute Error: 0.8182854655802024\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.988786671319576\n",
      "Root Mean Squared Error: 0.9943775295729363\n",
      "Mean Absolute Error: 0.8047404268100552\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0295679194713374\n",
      "Root Mean Squared Error: 1.0146762633822364\n",
      "Mean Absolute Error: 0.8218544985343275\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 1.039884451645314\n",
      "Root Mean Squared Error: 1.0197472489030377\n",
      "Mean Absolute Error: 0.824555156878068\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 1.0061174339052414\n",
      "Root Mean Squared Error: 1.0030540533317442\n",
      "Mean Absolute Error: 0.8138352944603489\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 1.008399932844093\n",
      "Root Mean Squared Error: 1.0041911834128463\n",
      "Mean Absolute Error: 0.8170004305769115\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 1.0028193674413508\n",
      "Root Mean Squared Error: 1.0014086915147835\n",
      "Mean Absolute Error: 0.814998713791393\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.7484201785513719\n",
      "Root Mean Squared Error: 0.8651128126154253\n",
      "Mean Absolute Error: 0.7217090677743223\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=500, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we change architecture to use `tanH` activation function and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.9601371051495889\n",
      "Root Mean Squared Error: 0.9798658607940114\n",
      "Mean Absolute Error: 0.7955025238462389\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9721314455819237\n",
      "Root Mean Squared Error: 0.9859672639504435\n",
      "Mean Absolute Error: 0.8000192989208814\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.9591478884520672\n",
      "Root Mean Squared Error: 0.979360959223956\n",
      "Mean Absolute Error: 0.794953325646777\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.898303457731608\n",
      "Root Mean Squared Error: 0.9477887199854238\n",
      "Mean Absolute Error: 0.7716470404433344\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9086596815429733\n",
      "Root Mean Squared Error: 0.9532364247881914\n",
      "Mean Absolute Error: 0.7761436736905984\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.8065224660740656\n",
      "Root Mean Squared Error: 0.898065958643387\n",
      "Mean Absolute Error: 0.737269836892417\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.7518228852487093\n",
      "Root Mean Squared Error: 0.8670772083550053\n",
      "Mean Absolute Error: 0.7190045198934326\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.6283567297905852\n",
      "Root Mean Squared Error: 0.792689554485604\n",
      "Mean Absolute Error: 0.6788152883177384\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6090525681559322\n",
      "Root Mean Squared Error: 0.7804182008102657\n",
      "Mean Absolute Error: 0.673826633648662\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=100, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.9898342755854097\n",
      "Root Mean Squared Error: 0.9949041539693206\n",
      "Mean Absolute Error: 0.802498866004221\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9760647518025803\n",
      "Root Mean Squared Error: 0.9879598938229124\n",
      "Mean Absolute Error: 0.8020116641218001\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.9943711375191621\n",
      "Root Mean Squared Error: 0.9971815970620206\n",
      "Mean Absolute Error: 0.8078522945700031\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0132672462151415\n",
      "Root Mean Squared Error: 1.0066117653868056\n",
      "Mean Absolute Error: 0.8136699311249833\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9974289774980071\n",
      "Root Mean Squared Error: 0.9987136614155265\n",
      "Mean Absolute Error: 0.8101543201134562\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.9633897015883526\n",
      "Root Mean Squared Error: 0.9815241726969095\n",
      "Mean Absolute Error: 0.7961584818144342\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.903979017683121\n",
      "Root Mean Squared Error: 0.95077811169753\n",
      "Mean Absolute Error: 0.774732777780537\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.9008427233926157\n",
      "Root Mean Squared Error: 0.9491273483535366\n",
      "Mean Absolute Error: 0.7784593939101635\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6085733978543773\n",
      "Root Mean Squared Error: 0.7801111445520934\n",
      "Mean Absolute Error: 0.6742935677697427\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.9355677376689863\n",
      "Root Mean Squared Error: 0.9672475058995946\n",
      "Mean Absolute Error: 0.7860356609362598\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 625us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9586691944357985\n",
      "Root Mean Squared Error: 0.9791165377194885\n",
      "Mean Absolute Error: 0.7933972677966467\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 608us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.9989807532284846\n",
      "Root Mean Squared Error: 0.9994902466900237\n",
      "Mean Absolute Error: 0.8093901485221542\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.9743182645594046\n",
      "Root Mean Squared Error: 0.9870756123820529\n",
      "Mean Absolute Error: 0.799986701470727\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9721882961864162\n",
      "Root Mean Squared Error: 0.9859960933930805\n",
      "Mean Absolute Error: 0.8000986069561302\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 559us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 1.017457148261263\n",
      "Root Mean Squared Error: 1.0086908090496627\n",
      "Mean Absolute Error: 0.8167336247357185\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9699155098494796\n",
      "Root Mean Squared Error: 0.9848428858703705\n",
      "Mean Absolute Error: 0.7987490308794323\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 609us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.9762158555297503\n",
      "Root Mean Squared Error: 0.9880363634653081\n",
      "Mean Absolute Error: 0.8007364671395855\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.9294859528198836\n",
      "Root Mean Squared Error: 0.9640985182126791\n",
      "Mean Absolute Error: 0.7870235994518374\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=500, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0163831433439834\n",
      "Root Mean Squared Error: 1.008158292801276\n",
      "Mean Absolute Error: 0.8178692252047989\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.0032280845652441\n",
      "Root Mean Squared Error: 1.001612741814542\n",
      "Mean Absolute Error: 0.8096088992533794\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 1.0266341237593566\n",
      "Root Mean Squared Error: 1.0132295513650185\n",
      "Mean Absolute Error: 0.8199684982928477\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0229081466882362\n",
      "Root Mean Squared Error: 1.0113892162210532\n",
      "Mean Absolute Error: 0.8190662236831544\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 558us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9587810240755759\n",
      "Root Mean Squared Error: 0.9791736434747291\n",
      "Mean Absolute Error: 0.7950923725366115\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 1.0481510970158592\n",
      "Root Mean Squared Error: 1.0237925068175968\n",
      "Mean Absolute Error: 0.837727216698615\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.8265703365916222\n",
      "Root Mean Squared Error: 0.9091591371105623\n",
      "Mean Absolute Error: 0.749211381412711\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.6782526155956704\n",
      "Root Mean Squared Error: 0.8235609361763526\n",
      "Mean Absolute Error: 0.7015550889391712\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.7122217897537836\n",
      "Root Mean Squared Error: 0.8439323371892936\n",
      "Mean Absolute Error: 0.7101522601982012\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=100, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.005254815175808\n",
      "Root Mean Squared Error: 1.002623964991765\n",
      "Mean Absolute Error: 0.8115824908259986\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.0380754777695524\n",
      "Root Mean Squared Error: 1.0188598911379094\n",
      "Mean Absolute Error: 0.822515812963892\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 1.0003056278226112\n",
      "Root Mean Squared Error: 1.0001528022370438\n",
      "Mean Absolute Error: 0.8087065350749999\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0126993289632271\n",
      "Root Mean Squared Error: 1.0063296323587154\n",
      "Mean Absolute Error: 0.8129271924610595\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9660500477637496\n",
      "Root Mean Squared Error: 0.9828784501471938\n",
      "Mean Absolute Error: 0.79838097744832\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.9425897281916392\n",
      "Root Mean Squared Error: 0.9708706032173594\n",
      "Mean Absolute Error: 0.7889857563360979\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9227104862617742\n",
      "Root Mean Squared Error: 0.9605782041363287\n",
      "Mean Absolute Error: 0.7841703073723599\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.8395621481601157\n",
      "Root Mean Squared Error: 0.916276240093628\n",
      "Mean Absolute Error: 0.7535531096758638\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6478718463111929\n",
      "Root Mean Squared Error: 0.8049048678640184\n",
      "Mean Absolute Error: 0.6850642155552903\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 0.8597283463535185\n",
      "Root Mean Squared Error: 0.9272153721512163\n",
      "Mean Absolute Error: 0.7542218912886078\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.027804733957583\n",
      "Root Mean Squared Error: 1.0138070496685172\n",
      "Mean Absolute Error: 0.8159655164148516\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.8818739542682212\n",
      "Root Mean Squared Error: 0.9390814417654207\n",
      "Mean Absolute Error: 0.7645031837413523\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 0.9948118916245138\n",
      "Root Mean Squared Error: 0.9974025724974414\n",
      "Mean Absolute Error: 0.8083845758202629\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 1.0001685839161611\n",
      "Root Mean Squared Error: 1.000084288405813\n",
      "Mean Absolute Error: 0.8103645737064309\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.9773639439016933\n",
      "Root Mean Squared Error: 0.9886171877434123\n",
      "Mean Absolute Error: 0.8015694610721171\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 1.0033330169072203\n",
      "Root Mean Squared Error: 1.0016651221377433\n",
      "Mean Absolute Error: 0.8114587047433253\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.9924198708582371\n",
      "Root Mean Squared Error: 0.9962027257833804\n",
      "Mean Absolute Error: 0.8072678211401759\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.9606619722038955\n",
      "Root Mean Squared Error: 0.9801336501742481\n",
      "Mean Absolute Error: 0.7969941279178714\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mse\", epochs=500, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat some steps with different number of neurons and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0592490216048542\n",
      "Root Mean Squared Error: 1.0291982421306667\n",
      "Mean Absolute Error: 0.8312799660952589\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9656492621872598\n",
      "Root Mean Squared Error: 0.9826745454051712\n",
      "Mean Absolute Error: 0.7986091994681949\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 1.0095991711611851\n",
      "Root Mean Squared Error: 1.00478812252195\n",
      "Mean Absolute Error: 0.8158796493503273\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0910990888166883\n",
      "Root Mean Squared Error: 1.0445568863478372\n",
      "Mean Absolute Error: 0.8478566277305261\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9589297629253216\n",
      "Root Mean Squared Error: 0.9792495917412075\n",
      "Mean Absolute Error: 0.796906242796378\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.8276234288283572\n",
      "Root Mean Squared Error: 0.9097381100230754\n",
      "Mean Absolute Error: 0.7458925886158767\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.7022743341213952\n",
      "Root Mean Squared Error: 0.8380180989223295\n",
      "Mean Absolute Error: 0.6997634538927291\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.698315906079685\n",
      "Root Mean Squared Error: 0.835652981852925\n",
      "Mean Absolute Error: 0.7024599982708166\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.6477127975422643\n",
      "Root Mean Squared Error: 0.804806062068536\n",
      "Mean Absolute Error: 0.6865839785507499\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=100, activation=\"relu\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0543008095942092\n",
      "Root Mean Squared Error: 1.026791512233233\n",
      "Mean Absolute Error: 0.8278744507241281\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 0.9806816619426849\n",
      "Root Mean Squared Error: 0.9902937250849795\n",
      "Mean Absolute Error: 0.8034202032653962\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 1.017950238416679\n",
      "Root Mean Squared Error: 1.0089352003060845\n",
      "Mean Absolute Error: 0.8165294097595907\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.0610589350662623\n",
      "Root Mean Squared Error: 1.0300771500554036\n",
      "Mean Absolute Error: 0.8314175248237748\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 1.018714115413834\n",
      "Root Mean Squared Error: 1.0093136853396143\n",
      "Mean Absolute Error: 0.8202770168184755\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 0.9740374836997627\n",
      "Root Mean Squared Error: 0.9869333734856486\n",
      "Mean Absolute Error: 0.8028839189899091\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9883135274532432\n",
      "Root Mean Squared Error: 0.9941395915329211\n",
      "Mean Absolute Error: 0.8090182312619821\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.649381971590421\n",
      "Root Mean Squared Error: 0.8058423987296902\n",
      "Mean Absolute Error: 0.6883349303543991\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.7695231444993298\n",
      "Root Mean Squared Error: 0.8772246830198804\n",
      "Mean Absolute Error: 0.7303104758903005\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=100, activation=\"relu\", num_neurons=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "For ratio 0.1\n",
      "Mean Squared Error: 1.0645231525524808\n",
      "Root Mean Squared Error: 1.0317573128175448\n",
      "Mean Absolute Error: 0.828240949328188\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "For ratio 0.2\n",
      "Mean Squared Error: 1.0558373005781476\n",
      "Root Mean Squared Error: 1.027539439913694\n",
      "Mean Absolute Error: 0.8282026713168356\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.3\n",
      "Mean Squared Error: 0.9598495994591898\n",
      "Root Mean Squared Error: 0.9797191431523576\n",
      "Mean Absolute Error: 0.794831564460817\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "For ratio 0.4\n",
      "Mean Squared Error: 1.014905275836611\n",
      "Root Mean Squared Error: 1.007425072070678\n",
      "Mean Absolute Error: 0.8169949774851295\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.5\n",
      "Mean Squared Error: 0.9657965416712664\n",
      "Root Mean Squared Error: 0.9827494806263021\n",
      "Mean Absolute Error: 0.7973835428106515\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599us/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.6\n",
      "Mean Squared Error: 1.0057880107317207\n",
      "Root Mean Squared Error: 1.0028898298077016\n",
      "Mean Absolute Error: 0.8154659980844302\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For ratio 0.7\n",
      "Mean Squared Error: 0.9783802091572834\n",
      "Root Mean Squared Error: 0.9891310374046927\n",
      "Mean Absolute Error: 0.804808255490347\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "For ratio 0.8\n",
      "Mean Squared Error: 0.7158267278831559\n",
      "Root Mean Squared Error: 0.8460654394803961\n",
      "Mean Absolute Error: 0.7081651656134461\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For ratio 0.9\n",
      "Mean Squared Error: 0.741787481969004\n",
      "Root Mean Squared Error: 0.8612708528500219\n",
      "Mean Absolute Error: 0.7191716768197028\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_ratio(loss=\"mae\", epochs=200, activation=\"relu\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use 90% train and 10% test split and see how it performs with different number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = np.linspace(2, 20, 19, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sin_predictions_layers(loss, epochs, activation, num_neurons):\n",
    "    accuracy_mses = []\n",
    "    accuracy_rmses = []\n",
    "    accuracy_maes = []\n",
    "\n",
    "    model_path = (\n",
    "        FILE_PATH\n",
    "        + \"layers/\"\n",
    "        + f\"model_epochs._neurons.{num_neurons}_epochs.{epochs}_activation.{activation}_loss.{loss}/\"\n",
    "    )\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    for i, num_layer in enumerate(NUM_LAYERS):\n",
    "        model = create_model(num_layer, num_neurons, activation, loss)\n",
    "        \n",
    "        file_path = model_path + f\"layers.{num_layer}/\"\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(file_path)        \n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_sin_values, y_sin_values, test_size=.9)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=epochs, verbose=0)\n",
    "\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "        error_stats(accuracy_mses, accuracy_rmses, accuracy_maes, y_test, y_test_pred, num_layer=num_layer)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(history.history[\"loss\"])\n",
    "        plt.title(f\"Loss function changes for layers {num_layer}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        loss_filename = f\"{file_path}loss_{i}.png\"\n",
    "        plt.savefig(loss_filename)\n",
    "        plt.close()\n",
    "\n",
    "        y_pred = model.predict(x_sin_values)\n",
    "        plt.figure()\n",
    "        plt.plot(x_sin_values, y_sin_values, label=\"True function\")\n",
    "        plt.plot(x_sin_values, y_pred, label=\"Estimated function\")\n",
    "        plt.title(f\"Function estimation for layers {num_layer}\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        sin_filename = f\"{file_path}sin_{i}.png\"\n",
    "        plt.savefig(sin_filename)\n",
    "        plt.close()\n",
    "        \n",
    "    frames_loss = np.stack(\n",
    "        [\n",
    "            iio.imread(f\"{model_path}layers.{NUM_LAYERS[i]}/loss_{i}.png\")\n",
    "            for i in range(len(NUM_LAYERS))\n",
    "        ]\n",
    "    )\n",
    "    iio.imwrite(f\"{model_path}loss.gif\", frames_loss, fps=0.5)\n",
    "    \n",
    "    frames_sin = np.stack(\n",
    "        [\n",
    "            iio.imread(f\"{model_path}layers.{NUM_LAYERS[i]}/sin_{i}.png\")\n",
    "            for i in range(len(NUM_LAYERS))\n",
    "        ]\n",
    "    )\n",
    "    iio.imwrite(f\"{model_path}sin.gif\", frames_sin, fps=0.5)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(NUM_LAYERS, accuracy_mses, label=\"Mean Squared Error\")\n",
    "    plt.plot(NUM_LAYERS, accuracy_rmses, label=\"Root Mean Squared Error\")\n",
    "    plt.plot(NUM_LAYERS, accuracy_maes, label=\"Mean Absolute Error\")\n",
    "    plt.title(\"Error changes for different layers\")\n",
    "    plt.xlabel(\"Number of layers\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc=\"upper right\")    \n",
    "    plt.savefig(f\"{model_path}errors.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `ReLU` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.5835343173541042\n",
      "Root Mean Squared Error: 0.7638941794215376\n",
      "Mean Absolute Error: 0.664318554917115\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.6313656478557885\n",
      "Root Mean Squared Error: 0.7945852049061752\n",
      "Mean Absolute Error: 0.6814913237038683\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 0.7920922008185761\n",
      "Root Mean Squared Error: 0.8899956184266168\n",
      "Mean Absolute Error: 0.7385287794224256\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.8907548703227405\n",
      "Root Mean Squared Error: 0.943798108878557\n",
      "Mean Absolute Error: 0.7725437805914217\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 6\n",
      "Mean Squared Error: 0.861390912941381\n",
      "Root Mean Squared Error: 0.9281114765702345\n",
      "Mean Absolute Error: 0.7620830541352158\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.871181684482489\n",
      "Root Mean Squared Error: 0.9333711397308624\n",
      "Mean Absolute Error: 0.7686005914618735\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.8542765720303386\n",
      "Root Mean Squared Error: 0.9242708326190644\n",
      "Mean Absolute Error: 0.756960060109272\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9586817993846881\n",
      "Root Mean Squared Error: 0.9791229745975161\n",
      "Mean Absolute Error: 0.7960043966560192\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 1.0024651806178169\n",
      "Root Mean Squared Error: 1.0012318316043576\n",
      "Mean Absolute Error: 0.8125010218708922\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.7778937798361958\n",
      "Root Mean Squared Error: 0.8819828682214841\n",
      "Mean Absolute Error: 0.7319873361905539\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 1.0019586753684335\n",
      "Root Mean Squared Error: 1.0009788586021353\n",
      "Mean Absolute Error: 0.8110322450712903\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.9188345114060316\n",
      "Root Mean Squared Error: 0.9585585591950194\n",
      "Mean Absolute Error: 0.7798280182894645\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.8738735940045932\n",
      "Root Mean Squared Error: 0.9348120634676219\n",
      "Mean Absolute Error: 0.762257191864087\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9409318703034695\n",
      "Root Mean Squared Error: 0.9700164278523686\n",
      "Mean Absolute Error: 0.7882102298944247\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.9468788098131702\n",
      "Root Mean Squared Error: 0.9730769804147924\n",
      "Mean Absolute Error: 0.7893858179879625\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.8717181258178073\n",
      "Root Mean Squared Error: 0.9336584631533135\n",
      "Mean Absolute Error: 0.7642043988654896\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.9408631125108767\n",
      "Root Mean Squared Error: 0.969980985643985\n",
      "Mean Absolute Error: 0.7877599537057585\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.9128889753957405\n",
      "Root Mean Squared Error: 0.9554522360619292\n",
      "Mean Absolute Error: 0.7780442655645726\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.8214838198918696\n",
      "Root Mean Squared Error: 0.9063574459846786\n",
      "Mean Absolute Error: 0.7456064827392344\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `ReLU` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.7035496990854\n",
      "Root Mean Squared Error: 0.8387786949400896\n",
      "Mean Absolute Error: 0.706094758133382\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.7378707596947921\n",
      "Root Mean Squared Error: 0.8589940393825746\n",
      "Mean Absolute Error: 0.7188055649304653\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 1.019512519691705\n",
      "Root Mean Squared Error: 1.0097091262792988\n",
      "Mean Absolute Error: 0.8188073975083748\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9353524469364997\n",
      "Root Mean Squared Error: 0.9671362090918216\n",
      "Mean Absolute Error: 0.7881472089123385\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9858572016164616\n",
      "Root Mean Squared Error: 0.9929034200849857\n",
      "Mean Absolute Error: 0.8043843527758704\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 1.0444464491252028\n",
      "Root Mean Squared Error: 1.0219816285654075\n",
      "Mean Absolute Error: 0.8279393619059816\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9816327968995501\n",
      "Root Mean Squared Error: 0.9907738374117224\n",
      "Mean Absolute Error: 0.8018203597606292\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9221849650958017\n",
      "Root Mean Squared Error: 0.9603046209905489\n",
      "Mean Absolute Error: 0.7813968496837487\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9564385641559763\n",
      "Root Mean Squared Error: 0.9779767707650199\n",
      "Mean Absolute Error: 0.7939284128885707\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.9243273102227716\n",
      "Root Mean Squared Error: 0.9614194247167942\n",
      "Mean Absolute Error: 0.7814420111914405\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.9526832718620132\n",
      "Root Mean Squared Error: 0.9760549533002807\n",
      "Mean Absolute Error: 0.7921785434976203\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.9957065476075263\n",
      "Root Mean Squared Error: 0.9978509646272464\n",
      "Mean Absolute Error: 0.808144930602225\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.9263816401709509\n",
      "Root Mean Squared Error: 0.9624872155883167\n",
      "Mean Absolute Error: 0.7832666258134569\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9729388692645353\n",
      "Root Mean Squared Error: 0.9863766366173397\n",
      "Mean Absolute Error: 0.8000007429927167\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.8262991078347939\n",
      "Root Mean Squared Error: 0.9090099602505981\n",
      "Mean Absolute Error: 0.7467528208522531\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.9105984369659932\n",
      "Root Mean Squared Error: 0.9542528160639575\n",
      "Mean Absolute Error: 0.7775400066389706\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 1.0072001660089687\n",
      "Root Mean Squared Error: 1.003593625930819\n",
      "Mean Absolute Error: 0.8120386473548326\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.91263521107038\n",
      "Root Mean Squared Error: 0.9553194288144569\n",
      "Mean Absolute Error: 0.7781800301463988\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.9600168038286221\n",
      "Root Mean Squared Error: 0.9798044722436319\n",
      "Mean Absolute Error: 0.7951804732393606\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 500 epochs and `ReLU` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.7094662238217051\n",
      "Root Mean Squared Error: 0.8422981798755742\n",
      "Mean Absolute Error: 0.7077528905610436\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.9341804865982307\n",
      "Root Mean Squared Error: 0.9665301271032531\n",
      "Mean Absolute Error: 0.7865560860411861\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 557us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 1.0006396585322757\n",
      "Root Mean Squared Error: 1.0003197781371094\n",
      "Mean Absolute Error: 0.8110429715277925\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9931480561188974\n",
      "Root Mean Squared Error: 0.9965681392252601\n",
      "Mean Absolute Error: 0.8076364680125442\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9313728240374163\n",
      "Root Mean Squared Error: 0.9650765897261296\n",
      "Mean Absolute Error: 0.7853671329566544\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.9641020545986116\n",
      "Root Mean Squared Error: 0.9818869866734214\n",
      "Mean Absolute Error: 0.7960905183164896\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.979242075783067\n",
      "Root Mean Squared Error: 0.9895666100789108\n",
      "Mean Absolute Error: 0.8025894818947734\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9924700471553785\n",
      "Root Mean Squared Error: 0.996227909243351\n",
      "Mean Absolute Error: 0.8076430995506522\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9602469662254328\n",
      "Root Mean Squared Error: 0.9799219184330111\n",
      "Mean Absolute Error: 0.7955564171883799\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 1.0247031510754394\n",
      "Root Mean Squared Error: 1.0122762227156377\n",
      "Mean Absolute Error: 0.820412010953802\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.9981227734096895\n",
      "Root Mean Squared Error: 0.9990609457934433\n",
      "Mean Absolute Error: 0.8094419689997409\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.9585151532003514\n",
      "Root Mean Squared Error: 0.9790378711777964\n",
      "Mean Absolute Error: 0.793681495357113\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.981352754325925\n",
      "Root Mean Squared Error: 0.9906325021550246\n",
      "Mean Absolute Error: 0.8022211941241278\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9930612352997166\n",
      "Root Mean Squared Error: 0.9965245783721125\n",
      "Mean Absolute Error: 0.8085329910336524\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 1.0194347987995522\n",
      "Root Mean Squared Error: 1.0096706387726406\n",
      "Mean Absolute Error: 0.8188949567982777\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 1.0076617653675009\n",
      "Root Mean Squared Error: 1.0038235728291605\n",
      "Mean Absolute Error: 0.8136458977847802\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.9627474075231556\n",
      "Root Mean Squared Error: 0.9811969259649949\n",
      "Mean Absolute Error: 0.7950493243895514\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.967627182054008\n",
      "Root Mean Squared Error: 0.983680426792161\n",
      "Mean Absolute Error: 0.7977424078728477\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.9917634860862863\n",
      "Root Mean Squared Error: 0.9958732279192398\n",
      "Mean Absolute Error: 0.8072933761751317\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=500, activation=\"relu\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `ReLU` activation function with 200 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.639705840543469\n",
      "Root Mean Squared Error: 0.7998161292093758\n",
      "Mean Absolute Error: 0.6822911493329451\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.6819468368207431\n",
      "Root Mean Squared Error: 0.8258007246429027\n",
      "Mean Absolute Error: 0.6988415536748366\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.7727325582133334\n",
      "Root Mean Squared Error: 0.879052079352147\n",
      "Mean Absolute Error: 0.7315770432342696\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 5\n",
      "Mean Squared Error: 0.9476597624006644\n",
      "Root Mean Squared Error: 0.9734781776704933\n",
      "Mean Absolute Error: 0.7903206780310292\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9120447902978468\n",
      "Root Mean Squared Error: 0.955010361356277\n",
      "Mean Absolute Error: 0.7785232543633953\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.8753403114628153\n",
      "Root Mean Squared Error: 0.9355962331384278\n",
      "Mean Absolute Error: 0.7657551269670356\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.8816089349901887\n",
      "Root Mean Squared Error: 0.9389403255746281\n",
      "Mean Absolute Error: 0.7659197478243183\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.7976852251865297\n",
      "Root Mean Squared Error: 0.8931322551484353\n",
      "Mean Absolute Error: 0.7372252526722383\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9443647592925902\n",
      "Root Mean Squared Error: 0.9717843172703449\n",
      "Mean Absolute Error: 0.7887860253518929\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 1.0938059379668967\n",
      "Root Mean Squared Error: 1.0458517762890192\n",
      "Mean Absolute Error: 0.8490911016546606\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.8796528524469479\n",
      "Root Mean Squared Error: 0.9378981034456504\n",
      "Mean Absolute Error: 0.7682860940902821\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.9313299066132771\n",
      "Root Mean Squared Error: 0.9650543542274067\n",
      "Mean Absolute Error: 0.7843382330366071\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.9276398818799687\n",
      "Root Mean Squared Error: 0.9631406345285037\n",
      "Mean Absolute Error: 0.7844049734514364\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.8759205049887863\n",
      "Root Mean Squared Error: 0.9359062479697345\n",
      "Mean Absolute Error: 0.7654333001943961\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.733196171604879\n",
      "Root Mean Squared Error: 0.8562687496369811\n",
      "Mean Absolute Error: 0.7171540324992367\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.8506041792878339\n",
      "Root Mean Squared Error: 0.9222820497482502\n",
      "Mean Absolute Error: 0.7542343921732108\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.7989756934854046\n",
      "Root Mean Squared Error: 0.8938544028450073\n",
      "Mean Absolute Error: 0.737713480178641\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.9068877268854926\n",
      "Root Mean Squared Error: 0.9523065298975392\n",
      "Mean Absolute Error: 0.7793786132766374\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.9259002864854217\n",
      "Root Mean Squared Error: 0.9622371259130577\n",
      "Mean Absolute Error: 0.7850733041661138\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"relu\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `ReLU` activation function with 500 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.6669334028229759\n",
      "Root Mean Squared Error: 0.8166599064622775\n",
      "Mean Absolute Error: 0.69438949758622\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 3\n",
      "Mean Squared Error: 0.6373190970563054\n",
      "Root Mean Squared Error: 0.7983226772779948\n",
      "Mean Absolute Error: 0.6823944508720118\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.8592074537478522\n",
      "Root Mean Squared Error: 0.9269344387538162\n",
      "Mean Absolute Error: 0.7584268658316401\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 1.0328407734860148\n",
      "Root Mean Squared Error: 1.0162877414817197\n",
      "Mean Absolute Error: 0.8246524734909387\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.7821483697980661\n",
      "Root Mean Squared Error: 0.8843915251731362\n",
      "Mean Absolute Error: 0.7308099120125172\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.9037144178101827\n",
      "Root Mean Squared Error: 0.9506389523947474\n",
      "Mean Absolute Error: 0.7748352571759248\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9079875028677695\n",
      "Root Mean Squared Error: 0.9528837824560609\n",
      "Mean Absolute Error: 0.7760857223708825\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9041765646279618\n",
      "Root Mean Squared Error: 0.9508819930085761\n",
      "Mean Absolute Error: 0.7771571371430921\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9523507377204904\n",
      "Root Mean Squared Error: 0.9758845924188425\n",
      "Mean Absolute Error: 0.7933429574665571\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.8978939926648601\n",
      "Root Mean Squared Error: 0.9475726846341974\n",
      "Mean Absolute Error: 0.7801362275572453\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 1.2218066610625322\n",
      "Root Mean Squared Error: 1.1053536362008913\n",
      "Mean Absolute Error: 0.898501943951928\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 1.0316638834672884\n",
      "Root Mean Squared Error: 1.0157085622693591\n",
      "Mean Absolute Error: 0.8284901807337888\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.9331963649603207\n",
      "Root Mean Squared Error: 0.9660208926106726\n",
      "Mean Absolute Error: 0.7835436803864246\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9545562711763128\n",
      "Root Mean Squared Error: 0.9770139564900354\n",
      "Mean Absolute Error: 0.7985307191810045\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.7491531325974763\n",
      "Root Mean Squared Error: 0.8655363265614426\n",
      "Mean Absolute Error: 0.7191160540532792\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.6501150320917705\n",
      "Root Mean Squared Error: 0.8062971115486961\n",
      "Mean Absolute Error: 0.6873422534545196\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.8416284831761182\n",
      "Root Mean Squared Error: 0.9174031192317357\n",
      "Mean Absolute Error: 0.7587006282758542\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.6786820925683543\n",
      "Root Mean Squared Error: 0.8238216388080337\n",
      "Mean Absolute Error: 0.6996017470477468\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5034845762456285\n",
      "Root Mean Squared Error: 0.7095664706323351\n",
      "Mean Absolute Error: 0.6364284004341485\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"relu\", num_neurons=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `ReLU` activation function with 200 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.7011690289051087\n",
      "Root Mean Squared Error: 0.8373583634890791\n",
      "Mean Absolute Error: 0.7034779499558519\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.8352231595250275\n",
      "Root Mean Squared Error: 0.9139054434267406\n",
      "Mean Absolute Error: 0.7504043282562255\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.9893722176134034\n",
      "Root Mean Squared Error: 0.9946719145594709\n",
      "Mean Absolute Error: 0.8067146458693577\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9684576962376216\n",
      "Root Mean Squared Error: 0.9841024825888925\n",
      "Mean Absolute Error: 0.7976679349950867\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9449525588366116\n",
      "Root Mean Squared Error: 0.9720867033534671\n",
      "Mean Absolute Error: 0.7894494923512979\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.9643459718070725\n",
      "Root Mean Squared Error: 0.982011187210753\n",
      "Mean Absolute Error: 0.7966552461052386\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9664446718787572\n",
      "Root Mean Squared Error: 0.9830791788451005\n",
      "Mean Absolute Error: 0.7976946697455136\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9728862700292576\n",
      "Root Mean Squared Error: 0.9863499734015597\n",
      "Mean Absolute Error: 0.799213076587708\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9374319744018734\n",
      "Root Mean Squared Error: 0.9682107076467774\n",
      "Mean Absolute Error: 0.785118445937128\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.9486904106476168\n",
      "Root Mean Squared Error: 0.9740073976349547\n",
      "Mean Absolute Error: 0.7898669203518243\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 1.009814205424447\n",
      "Root Mean Squared Error: 1.0048951216044624\n",
      "Mean Absolute Error: 0.8133228877142304\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 1.0048621847653258\n",
      "Root Mean Squared Error: 1.0024281444399523\n",
      "Mean Absolute Error: 0.8116152527486817\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 1.0057032172190747\n",
      "Root Mean Squared Error: 1.0028475543267155\n",
      "Mean Absolute Error: 0.814096045965103\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.8836452614383874\n",
      "Root Mean Squared Error: 0.9400240749248858\n",
      "Mean Absolute Error: 0.7702863865587576\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.9912373799549284\n",
      "Root Mean Squared Error: 0.9956090497554391\n",
      "Mean Absolute Error: 0.8081015384619253\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 1.0186760239091615\n",
      "Root Mean Squared Error: 1.00929481516015\n",
      "Mean Absolute Error: 0.8167359053225847\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.9990789456546412\n",
      "Root Mean Squared Error: 0.9995393667358186\n",
      "Mean Absolute Error: 0.8110619777986838\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.93807027692287\n",
      "Root Mean Squared Error: 0.9685402815179501\n",
      "Mean Absolute Error: 0.7874434330804895\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.9231390046897596\n",
      "Root Mean Squared Error: 0.9608012305829753\n",
      "Mean Absolute Error: 0.7816089646256285\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"relu\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `ReLU` activation function with 500 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.6013204126126922\n",
      "Root Mean Squared Error: 0.7754485235092605\n",
      "Mean Absolute Error: 0.6702708141196706\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.9595760008911401\n",
      "Root Mean Squared Error: 0.9795795020778764\n",
      "Mean Absolute Error: 0.7950379113873851\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 0.9737927667372624\n",
      "Root Mean Squared Error: 0.9868093872360875\n",
      "Mean Absolute Error: 0.8001027543141086\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 5\n",
      "Mean Squared Error: 1.0033772074011866\n",
      "Root Mean Squared Error: 1.0016871804117224\n",
      "Mean Absolute Error: 0.8118736065215184\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 1.0217388127619103\n",
      "Root Mean Squared Error: 1.0108109678678354\n",
      "Mean Absolute Error: 0.8180453299423969\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.98490126356297\n",
      "Root Mean Squared Error: 0.9924219181189874\n",
      "Mean Absolute Error: 0.8030565534918243\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 1.0147331814600644\n",
      "Root Mean Squared Error: 1.0073396554589045\n",
      "Mean Absolute Error: 0.8152582735622931\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.999519993226625\n",
      "Root Mean Squared Error: 0.9997599678055853\n",
      "Mean Absolute Error: 0.8107227784185355\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9299000405025486\n",
      "Root Mean Squared Error: 0.9643132481214538\n",
      "Mean Absolute Error: 0.7881971510032938\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.9709072642754849\n",
      "Root Mean Squared Error: 0.9853462661803133\n",
      "Mean Absolute Error: 0.7975482406953127\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.9513862060192432\n",
      "Root Mean Squared Error: 0.97539028394753\n",
      "Mean Absolute Error: 0.7911165500273645\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.9905781105861973\n",
      "Root Mean Squared Error: 0.9952779062082094\n",
      "Mean Absolute Error: 0.8059235065422612\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.8688496350691476\n",
      "Root Mean Squared Error: 0.9321210409969016\n",
      "Mean Absolute Error: 0.7604750408883644\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9653767184253408\n",
      "Root Mean Squared Error: 0.9825358611396028\n",
      "Mean Absolute Error: 0.7958523034359725\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5018615063850798\n",
      "Root Mean Squared Error: 0.7084218421146258\n",
      "Mean Absolute Error: 0.6353211376848249\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.6521150940592737\n",
      "Root Mean Squared Error: 0.8075364351280218\n",
      "Mean Absolute Error: 0.6869658158080814\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.5641271291041403\n",
      "Root Mean Squared Error: 0.7510839694096395\n",
      "Mean Absolute Error: 0.6589119593196442\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.6469008413532588\n",
      "Root Mean Squared Error: 0.8043014617376116\n",
      "Mean Absolute Error: 0.6839699240235498\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.7539429679573858\n",
      "Root Mean Squared Error: 0.8682988932144194\n",
      "Mean Absolute Error: 0.7215015829867376\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `tanH` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.5586102330882908\n",
      "Root Mean Squared Error: 0.747402323443198\n",
      "Mean Absolute Error: 0.657703907601622\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.5916907576046735\n",
      "Root Mean Squared Error: 0.7692143768837615\n",
      "Mean Absolute Error: 0.6664513631508863\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 0.8683462734372318\n",
      "Root Mean Squared Error: 0.93185099315139\n",
      "Mean Absolute Error: 0.7616536024565196\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9454430183691465\n",
      "Root Mean Squared Error: 0.9723389421231398\n",
      "Mean Absolute Error: 0.7901290945790794\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 1.0483562800598387\n",
      "Root Mean Squared Error: 1.0238927092522139\n",
      "Mean Absolute Error: 0.8285278170084394\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 1.009139970419748\n",
      "Root Mean Squared Error: 1.0045595902781217\n",
      "Mean Absolute Error: 0.813051864988138\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9653914129953695\n",
      "Root Mean Squared Error: 0.9825433389908913\n",
      "Mean Absolute Error: 0.7963368105311114\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9228535181215771\n",
      "Root Mean Squared Error: 0.9606526521701677\n",
      "Mean Absolute Error: 0.779750063021446\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9951527714104489\n",
      "Root Mean Squared Error: 0.9975734416124202\n",
      "Mean Absolute Error: 0.8062534134741525\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.8328689285205974\n",
      "Root Mean Squared Error: 0.9126165287351513\n",
      "Mean Absolute Error: 0.7481146248184214\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.9973025117574188\n",
      "Root Mean Squared Error: 0.9986503450945274\n",
      "Mean Absolute Error: 0.8077080653057424\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.6130433806840192\n",
      "Root Mean Squared Error: 0.7829708683495313\n",
      "Mean Absolute Error: 0.673633843333185\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.8589603175804357\n",
      "Root Mean Squared Error: 0.9268011208346889\n",
      "Mean Absolute Error: 0.7564542846748826\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.6038048059276727\n",
      "Root Mean Squared Error: 0.7770487796320593\n",
      "Mean Absolute Error: 0.6735443811611127\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5340858105415429\n",
      "Root Mean Squared Error: 0.7308117476762007\n",
      "Mean Absolute Error: 0.6452575891189921\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.8215574591230341\n",
      "Root Mean Squared Error: 0.9063980687992633\n",
      "Mean Absolute Error: 0.7532745714587323\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.5802033818075396\n",
      "Root Mean Squared Error: 0.7617108255811648\n",
      "Mean Absolute Error: 0.6608645945518058\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.5972838760142392\n",
      "Root Mean Squared Error: 0.7728414300580936\n",
      "Mean Absolute Error: 0.668199280196995\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.7480577842604149\n",
      "Root Mean Squared Error: 0.86490333810225\n",
      "Mean Absolute Error: 0.7241766234021281\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `tanH` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.6141852677636929\n",
      "Root Mean Squared Error: 0.7836997306135131\n",
      "Mean Absolute Error: 0.6766107073729337\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.9334937408108627\n",
      "Root Mean Squared Error: 0.9661747982693725\n",
      "Mean Absolute Error: 0.7847469356785943\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 4\n",
      "Mean Squared Error: 0.9994813792784775\n",
      "Root Mean Squared Error: 0.9997406560095861\n",
      "Mean Absolute Error: 0.8095909831395426\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9948581373865111\n",
      "Root Mean Squared Error: 0.9974257553254333\n",
      "Mean Absolute Error: 0.809264148402119\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9793921489675668\n",
      "Root Mean Squared Error: 0.9896424349064498\n",
      "Mean Absolute Error: 0.8049205272680793\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.9884269491048104\n",
      "Root Mean Squared Error: 0.9941966350299172\n",
      "Mean Absolute Error: 0.8054293195036136\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9572109897922071\n",
      "Root Mean Squared Error: 0.9783716010760978\n",
      "Mean Absolute Error: 0.7938175299233372\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 1.0128075388125823\n",
      "Root Mean Squared Error: 1.0063833955370003\n",
      "Mean Absolute Error: 0.8144024862642645\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9785389333209621\n",
      "Root Mean Squared Error: 0.9892112682945752\n",
      "Mean Absolute Error: 0.8004105948563033\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.9481318030937668\n",
      "Root Mean Squared Error: 0.9737205980638218\n",
      "Mean Absolute Error: 0.7902628532496292\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 1.0217741983054764\n",
      "Root Mean Squared Error: 1.0108284712578472\n",
      "Mean Absolute Error: 0.8179992697775551\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.960557768834269\n",
      "Root Mean Squared Error: 0.9800804909976879\n",
      "Mean Absolute Error: 0.7931548170261015\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.5207422357769388\n",
      "Root Mean Squared Error: 0.7216247194885571\n",
      "Mean Absolute Error: 0.6418204976866955\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.5983603977946224\n",
      "Root Mean Squared Error: 0.773537586542905\n",
      "Mean Absolute Error: 0.671753808855802\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5954053785343023\n",
      "Root Mean Squared Error: 0.7716251541612043\n",
      "Mean Absolute Error: 0.6688227062293505\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.8973483383284885\n",
      "Root Mean Squared Error: 0.9472847187242537\n",
      "Mean Absolute Error: 0.768879542859748\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.6139325940815887\n",
      "Root Mean Squared Error: 0.7835385083590907\n",
      "Mean Absolute Error: 0.6744988711509355\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.6109478737088484\n",
      "Root Mean Squared Error: 0.7816315460041569\n",
      "Mean Absolute Error: 0.673581763865416\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5670284294852275\n",
      "Root Mean Squared Error: 0.7530129012740934\n",
      "Mean Absolute Error: 0.6598688920400301\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 500 epochs and `tanH` activation function with 100 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.849668308576126\n",
      "Root Mean Squared Error: 0.9217745432458666\n",
      "Mean Absolute Error: 0.7562913866471344\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.9973232096789865\n",
      "Root Mean Squared Error: 0.9986607079879465\n",
      "Mean Absolute Error: 0.8088657934421142\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 1.0312661006041448\n",
      "Root Mean Squared Error: 1.0155127279380327\n",
      "Mean Absolute Error: 0.8240021711980385\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 1.0164809089562645\n",
      "Root Mean Squared Error: 1.0082067788684346\n",
      "Mean Absolute Error: 0.8163596345229037\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9979942487333828\n",
      "Root Mean Squared Error: 0.9989966209819645\n",
      "Mean Absolute Error: 0.8104520259021966\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.9590543266660526\n",
      "Root Mean Squared Error: 0.9793131913060564\n",
      "Mean Absolute Error: 0.7945688414848812\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9640820524665311\n",
      "Root Mean Squared Error: 0.9818768010634181\n",
      "Mean Absolute Error: 0.7973603167382202\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9987888941234379\n",
      "Root Mean Squared Error: 0.9993942636034279\n",
      "Mean Absolute Error: 0.8093486547276952\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 1.000133296590607\n",
      "Root Mean Squared Error: 1.0000666460744538\n",
      "Mean Absolute Error: 0.8104706623999761\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 1.0331585240152035\n",
      "Root Mean Squared Error: 1.016444058477988\n",
      "Mean Absolute Error: 0.8230911633655346\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.988943202350284\n",
      "Root Mean Squared Error: 0.9944562345072225\n",
      "Mean Absolute Error: 0.8041818264017607\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.7651598220440834\n",
      "Root Mean Squared Error: 0.8747341436368443\n",
      "Mean Absolute Error: 0.7258449354040849\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.6601328690585966\n",
      "Root Mean Squared Error: 0.8124856116009666\n",
      "Mean Absolute Error: 0.6908277347194008\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.9666959683463204\n",
      "Root Mean Squared Error: 0.9832069814369304\n",
      "Mean Absolute Error: 0.7952756999598459\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5438483394932072\n",
      "Root Mean Squared Error: 0.737460737594353\n",
      "Mean Absolute Error: 0.6528359076062673\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.6554053265102296\n",
      "Root Mean Squared Error: 0.8095710756383467\n",
      "Mean Absolute Error: 0.6867754664413774\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.763294341570754\n",
      "Root Mean Squared Error: 0.8736671800924847\n",
      "Mean Absolute Error: 0.7284861422862083\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.5760054085840274\n",
      "Root Mean Squared Error: 0.7589502016496388\n",
      "Mean Absolute Error: 0.6602943365093025\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5036255511683305\n",
      "Root Mean Squared Error: 0.7096658024509357\n",
      "Mean Absolute Error: 0.6358374698972165\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=500, activation=\"tanh\", num_neurons=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `tanH` activation function with 200 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.6209571225244604\n",
      "Root Mean Squared Error: 0.7880083264309207\n",
      "Mean Absolute Error: 0.6795701882102656\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 3\n",
      "Mean Squared Error: 0.6471684007589333\n",
      "Root Mean Squared Error: 0.8044677748418101\n",
      "Mean Absolute Error: 0.6854775258978507\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.9045361651353077\n",
      "Root Mean Squared Error: 0.9510710620849042\n",
      "Mean Absolute Error: 0.7749474771453515\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9872080298578289\n",
      "Root Mean Squared Error: 0.9935834287355183\n",
      "Mean Absolute Error: 0.809125857956594\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.973015985079648\n",
      "Root Mean Squared Error: 0.9864157262937611\n",
      "Mean Absolute Error: 0.7996022369914751\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.6904087052989757\n",
      "Root Mean Squared Error: 0.8309083615531714\n",
      "Mean Absolute Error: 0.7023880384714801\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.6000726540865114\n",
      "Root Mean Squared Error: 0.7746435658330297\n",
      "Mean Absolute Error: 0.6701306284284355\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9551005957950116\n",
      "Root Mean Squared Error: 0.97729248221554\n",
      "Mean Absolute Error: 0.7911159607761579\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.6482611997507786\n",
      "Root Mean Squared Error: 0.8051466945537183\n",
      "Mean Absolute Error: 0.6861042406326037\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.5916361385320301\n",
      "Root Mean Squared Error: 0.7691788729106059\n",
      "Mean Absolute Error: 0.6652485325339206\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.6333140677913236\n",
      "Root Mean Squared Error: 0.7958103214908209\n",
      "Mean Absolute Error: 0.6795065387437775\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.7014611673391065\n",
      "Root Mean Squared Error: 0.8375327858293706\n",
      "Mean Absolute Error: 0.6974872983907556\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.512666115356664\n",
      "Root Mean Squared Error: 0.716007063761709\n",
      "Mean Absolute Error: 0.6409165862831643\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.8268800300093713\n",
      "Root Mean Squared Error: 0.9093294397573254\n",
      "Mean Absolute Error: 0.7583394772662266\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5016349361985966\n",
      "Root Mean Squared Error: 0.708261912147333\n",
      "Mean Absolute Error: 0.6336712854070119\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.7466662520016804\n",
      "Root Mean Squared Error: 0.8640985198469445\n",
      "Mean Absolute Error: 0.7196510771779058\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.6524307459200175\n",
      "Root Mean Squared Error: 0.8077318527333297\n",
      "Mean Absolute Error: 0.6905411254303836\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.5058790809885124\n",
      "Root Mean Squared Error: 0.7112517704642375\n",
      "Mean Absolute Error: 0.6392674581814793\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5014641792561474\n",
      "Root Mean Squared Error: 0.7081413554200513\n",
      "Mean Absolute Error: 0.6341160997661339\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"tanh\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 100 epochs and `tanh` activation function with 500 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 2\n",
      "Mean Squared Error: 0.5687296904799997\n",
      "Root Mean Squared Error: 0.7541416912490647\n",
      "Mean Absolute Error: 0.6577398490123261\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 3\n",
      "Mean Squared Error: 0.6073999014306912\n",
      "Root Mean Squared Error: 0.7793586474985\n",
      "Mean Absolute Error: 0.6720936446616086\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.6122926824794108\n",
      "Root Mean Squared Error: 0.782491330609746\n",
      "Mean Absolute Error: 0.6732414667276513\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.6596790240384267\n",
      "Root Mean Squared Error: 0.8122062693912345\n",
      "Mean Absolute Error: 0.6899997297669287\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9162841421288148\n",
      "Root Mean Squared Error: 0.957227319986645\n",
      "Mean Absolute Error: 0.7818490686106697\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.6036589759647365\n",
      "Root Mean Squared Error: 0.7769549381815759\n",
      "Mean Absolute Error: 0.6680208083124458\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.8041338483366988\n",
      "Root Mean Squared Error: 0.8967351048869999\n",
      "Mean Absolute Error: 0.746127698646791\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.7462508936131795\n",
      "Root Mean Squared Error: 0.8638581443808813\n",
      "Mean Absolute Error: 0.7223271564158245\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.6808360908853859\n",
      "Root Mean Squared Error: 0.8251279239520293\n",
      "Mean Absolute Error: 0.7038874920960345\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.6586856466217087\n",
      "Root Mean Squared Error: 0.811594508742949\n",
      "Mean Absolute Error: 0.6963164396119375\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.5081910996237565\n",
      "Root Mean Squared Error: 0.7128752342617581\n",
      "Mean Absolute Error: 0.6439468310949031\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.5897807968695334\n",
      "Root Mean Squared Error: 0.7679718724468582\n",
      "Mean Absolute Error: 0.6675402344109834\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.5140415609774837\n",
      "Root Mean Squared Error: 0.7169669176311301\n",
      "Mean Absolute Error: 0.6392056778968103\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.5509263495125222\n",
      "Root Mean Squared Error: 0.7422441306689613\n",
      "Mean Absolute Error: 0.6518183337202934\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5475753421442064\n",
      "Root Mean Squared Error: 0.7399833390990681\n",
      "Mean Absolute Error: 0.6551277796274244\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.504817687689805\n",
      "Root Mean Squared Error: 0.7105052341044399\n",
      "Mean Absolute Error: 0.6396253764843326\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.5258619303791424\n",
      "Root Mean Squared Error: 0.7251633818520778\n",
      "Mean Absolute Error: 0.6437922962232577\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.5000985320299226\n",
      "Root Mean Squared Error: 0.7071764504209134\n",
      "Mean Absolute Error: 0.6336716689423901\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5146520812592854\n",
      "Root Mean Squared Error: 0.717392557292927\n",
      "Mean Absolute Error: 0.6420320027299004\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=100, activation=\"tanh\", num_neurons=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `tanH` activation function with 200 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 2\n",
      "Mean Squared Error: 0.681196616989953\n",
      "Root Mean Squared Error: 0.8253463618323843\n",
      "Mean Absolute Error: 0.6974701716496226\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "For layer 3\n",
      "Mean Squared Error: 0.9560383952725715\n",
      "Root Mean Squared Error: 0.9777721591825835\n",
      "Mean Absolute Error: 0.7934092188884971\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 0.9235562523533475\n",
      "Root Mean Squared Error: 0.9610183413199498\n",
      "Mean Absolute Error: 0.7817346701222206\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9417893369488165\n",
      "Root Mean Squared Error: 0.9704583128341044\n",
      "Mean Absolute Error: 0.7882652079415247\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.9442973147360986\n",
      "Root Mean Squared Error: 0.9717496152487525\n",
      "Mean Absolute Error: 0.7899463263513146\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 1.0219244075604526\n",
      "Root Mean Squared Error: 1.0109027685986682\n",
      "Mean Absolute Error: 0.8174534570664528\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.9726603498536779\n",
      "Root Mean Squared Error: 0.9862354434178878\n",
      "Mean Absolute Error: 0.7989368295262225\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.9781579648105134\n",
      "Root Mean Squared Error: 0.9890186877964002\n",
      "Mean Absolute Error: 0.8009968588615466\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.9190098157225931\n",
      "Root Mean Squared Error: 0.9586499964651296\n",
      "Mean Absolute Error: 0.7791649639540252\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.6913612403156335\n",
      "Root Mean Squared Error: 0.8314813529572588\n",
      "Mean Absolute Error: 0.7053427149549413\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.531772533914982\n",
      "Root Mean Squared Error: 0.7292273540638626\n",
      "Mean Absolute Error: 0.6477589036409508\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.7199688944557443\n",
      "Root Mean Squared Error: 0.8485098081081587\n",
      "Mean Absolute Error: 0.7116794254827732\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.6253064054296323\n",
      "Root Mean Squared Error: 0.7907631791058763\n",
      "Mean Absolute Error: 0.679300493864783\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.49823286376684\n",
      "Root Mean Squared Error: 0.7058561211513575\n",
      "Mean Absolute Error: 0.6345822634527742\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.6044810099455481\n",
      "Root Mean Squared Error: 0.7774837682842956\n",
      "Mean Absolute Error: 0.6693995408271689\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.5027613526458833\n",
      "Root Mean Squared Error: 0.7090566639175485\n",
      "Mean Absolute Error: 0.6380733188712524\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.5114921768974137\n",
      "Root Mean Squared Error: 0.7151868125863435\n",
      "Mean Absolute Error: 0.6373916323672485\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.5094559580619302\n",
      "Root Mean Squared Error: 0.713761835672047\n",
      "Mean Absolute Error: 0.636660855422843\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5290287992983906\n",
      "Root Mean Squared Error: 0.7273436596949139\n",
      "Mean Absolute Error: 0.6470275304615309\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with Mean Squared Error loss and 200 epochs and `tanH` activation function with 500 neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "For layer 2\n",
      "Mean Squared Error: 0.8375493375908025\n",
      "Root Mean Squared Error: 0.9151772164945993\n",
      "Mean Absolute Error: 0.7525158335209493\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 3\n",
      "Mean Squared Error: 0.9405697698566822\n",
      "Root Mean Squared Error: 0.9698297633382273\n",
      "Mean Absolute Error: 0.7885089144316629\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 4\n",
      "Mean Squared Error: 1.0382587857176275\n",
      "Root Mean Squared Error: 1.018949844554494\n",
      "Mean Absolute Error: 0.8264233215955279\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "For layer 5\n",
      "Mean Squared Error: 0.9861151688744426\n",
      "Root Mean Squared Error: 0.9930333171019201\n",
      "Mean Absolute Error: 0.8052553303631838\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 6\n",
      "Mean Squared Error: 0.978239744575152\n",
      "Root Mean Squared Error: 0.9890600308247988\n",
      "Mean Absolute Error: 0.801721876396055\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "For layer 7\n",
      "Mean Squared Error: 0.8809331682056472\n",
      "Root Mean Squared Error: 0.9385804005015485\n",
      "Mean Absolute Error: 0.7750467799838294\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 8\n",
      "Mean Squared Error: 0.5182795785533892\n",
      "Root Mean Squared Error: 0.7199163691383808\n",
      "Mean Absolute Error: 0.6439583286718809\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 9\n",
      "Mean Squared Error: 0.5684466928469343\n",
      "Root Mean Squared Error: 0.753954038948618\n",
      "Mean Absolute Error: 0.6587637678266237\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 10\n",
      "Mean Squared Error: 0.5452019936432106\n",
      "Root Mean Squared Error: 0.7383779476956301\n",
      "Mean Absolute Error: 0.6477687845422717\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "For layer 11\n",
      "Mean Squared Error: 0.5080253972177388\n",
      "Root Mean Squared Error: 0.712759003603419\n",
      "Mean Absolute Error: 0.6414009708939524\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "For layer 12\n",
      "Mean Squared Error: 0.5173641280525212\n",
      "Root Mean Squared Error: 0.7192802847656268\n",
      "Mean Absolute Error: 0.640139615125827\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "For layer 13\n",
      "Mean Squared Error: 0.5081822648767448\n",
      "Root Mean Squared Error: 0.7128690376757464\n",
      "Mean Absolute Error: 0.6431296879675447\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "For layer 14\n",
      "Mean Squared Error: 0.5557587404306071\n",
      "Root Mean Squared Error: 0.7454922805975975\n",
      "Mean Absolute Error: 0.6535874044956221\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "For layer 15\n",
      "Mean Squared Error: 0.5107095800606276\n",
      "Root Mean Squared Error: 0.714639475582358\n",
      "Mean Absolute Error: 0.6406883169148871\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "For layer 16\n",
      "Mean Squared Error: 0.5086681964555031\n",
      "Root Mean Squared Error: 0.7132097843240116\n",
      "Mean Absolute Error: 0.6424751340505562\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "For layer 17\n",
      "Mean Squared Error: 0.5120322139324263\n",
      "Root Mean Squared Error: 0.7155642626154735\n",
      "Mean Absolute Error: 0.6404401796993594\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "For layer 18\n",
      "Mean Squared Error: 0.501779745777448\n",
      "Root Mean Squared Error: 0.708364133604637\n",
      "Mean Absolute Error: 0.6367329884311558\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "For layer 19\n",
      "Mean Squared Error: 0.6659096184501403\n",
      "Root Mean Squared Error: 0.8160328537811087\n",
      "Mean Absolute Error: 0.6928615630670772\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "For layer 20\n",
      "Mean Squared Error: 0.5112525825863946\n",
      "Root Mean Squared Error: 0.7150192882617885\n",
      "Mean Absolute Error: 0.6387691035293561\n",
      "\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "plot_sin_predictions_layers(loss=\"mse\", epochs=200, activation=\"tanh\", num_neurons=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We now analyze overfitting on a classification problem using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_minst_train, y_minst_train), (x_minst_test, y_minst_test) = mnist.load_data()\n",
    "assert x_minst_train.shape == (60000, 28, 28)\n",
    "assert x_minst_test.shape == (10000, 28, 28)\n",
    "assert y_minst_train.shape == (60000,)\n",
    "assert y_minst_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then searched for optimal neural network for MNIST on the internet \n",
    "\n",
    "links:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
    "\n",
    "https://medium.com/@martin_stoyanov/optimizing-hyperparameters-for-the-mnist-dataset-in-javascript-4cb8c17df940\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb?hl=en#scrollTo=wYG5qXpP5a9n   # This is the original notebook READMEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/datasets/keras_example # README TOO\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/prashant111/mnist-deep-neural-network-with-keras # AND ME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
