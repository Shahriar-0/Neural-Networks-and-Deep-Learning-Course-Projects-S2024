{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimised Neural Network\n",
    "\n",
    "In this notebook, we will look into common problem in neural networks, which is overfitting. We will use the a sin(x) and popular MNIST dataset for this purpose, and use different architectures and different train test splits to see how our model performs. Then we try to find optimal hyperparameters for our model to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end.\n",
    "# MATIN read the links in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Overfitting is a common problem in neural networks, where the model performs well on the training data but poorly on the test data. This is because the model has learned the noise in the training data, rather than the underlying pattern. Common reasons for overfitting include using a model that is too complex, not having enough training data, and training for too many epochs. There are several ways to reduce overfitting, such as using dropout, early stopping, and data augmentation. In this notebook, we will explore some of these techniques and see how they can be used to improve the performance of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "At first we will use a simple sin(x) dataset to demonstrate overfitting. We will generate 1000 data points between 0 and 4*pi and use this as our training data. We will then use different architectures and train test splits to see how our model performs. We will then use the popular MNIST dataset to further demonstrate overfitting and explore different techniques to reduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression \n",
    "\n",
    "We first analyze overfitting on a simple regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sin_values = np.linspace(0, 4 * np.pi, 1000)\n",
    "y_sin_values = np.sin(x_sin_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We now analyze overfitting on a classification problem using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_minst_train, y_minst_train), (x_minst_test, y_minst_test) = mnist.load_data()\n",
    "assert x_minst_train.shape == (60000, 28, 28)\n",
    "assert x_minst_test.shape == (10000, 28, 28)\n",
    "assert y_minst_train.shape == (60000,)\n",
    "assert y_minst_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then searched for optimal neural network for MNIST on the internet \n",
    "\n",
    "links:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
    "\n",
    "https://medium.com/@martin_stoyanov/optimizing-hyperparameters-for-the-mnist-dataset-in-javascript-4cb8c17df940\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb?hl=en#scrollTo=wYG5qXpP5a9n   # This is the original notebook READMEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/datasets/keras_example # README TOO\n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/prashant111/mnist-deep-neural-network-with-keras # AND ME"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
