{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "from keras import layers, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Flatten,\n",
    "    Reshape,\n",
    "    Conv2DTranspose,\n",
    "    LeakyReLU,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Rescaling,\n",
    ")\n",
    "\n",
    "from six.moves import cPickle\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 256, 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63565 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "anime_train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"../data/anime_face/images\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train_ds, anime_val_ds = anime_train_ds.take(50000), anime_train_ds.skip(50000)\n",
    "anime_val_ds, anime_test_ds = anime_val_ds.take(10000), anime_val_ds.skip(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filename):\n",
    "    with open(filename, \"rb\") as fo:\n",
    "        return cPickle.load(fo, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_flattened_image_batch(flat_image_batch):\n",
    "    return flat_image_batch.reshape(-1, 3, 32, 32).transpose([0, 2, 3, 1])  # convert from NCHW to NHWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_and_normalise_images(data_dict):\n",
    "    images = data_dict[\"images\"]\n",
    "    data_dict[\"images\"] = (tf.cast(images, tf.float32) / 255.0) - 0.5\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train_ds = anime_train_ds.map(lambda x: tf.image.resize(x, (32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack(h, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "  for i in range(num_residual_layers):\n",
    "    h_i = tf.nn.relu(h)\n",
    "\n",
    "    h_i = Conv2D(\n",
    "        output_channels=num_residual_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"res3x3_%d\" % i)(h_i)\n",
    "    h_i = tf.nn.relu(h_i)\n",
    "\n",
    "    h_i = Conv2D(\n",
    "        output_channels=num_hiddens,\n",
    "        kernel_shape=(1, 1),\n",
    "        stride=(1, 1),\n",
    "        name=\"res1x1_%d\" % i)(h_i)\n",
    "    h += h_i\n",
    "  return tf.nn.relu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='encoder'):\n",
    "    super(Encoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "    \n",
    "  def _build(self, x):\n",
    "    h = Conv2D(\n",
    "        output_channels=self._num_hiddens / 2,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_1\")(x)\n",
    "    h = tf.nn.relu(h)\n",
    "\n",
    "    h = Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"enc_2\")(h)\n",
    "    h = tf.nn.relu(h)\n",
    "\n",
    "    h = Conv2D(\n",
    "        output_channels=self._num_hiddens,\n",
    "        kernel_shape=(3, 3),\n",
    "        stride=(1, 1),\n",
    "        name=\"enc_3\")(h)\n",
    "\n",
    "    h = residual_stack(\n",
    "        h,\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "  def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
    "               name='decoder'):\n",
    "    super(Decoder, self).__init__(name=name)\n",
    "    self._num_hiddens = num_hiddens\n",
    "    self._num_residual_layers = num_residual_layers\n",
    "    self._num_residual_hiddens = num_residual_hiddens\n",
    "  \n",
    "  def _build(self, x):\n",
    "    h = Conv2D(\n",
    "      output_channels=self._num_hiddens,\n",
    "      kernel_shape=(3, 3),\n",
    "      stride=(1, 1),\n",
    "      name=\"dec_1\")(x)\n",
    "\n",
    "    h = residual_stack(\n",
    "        h,\n",
    "        self._num_hiddens,\n",
    "        self._num_residual_layers,\n",
    "        self._num_residual_hiddens)\n",
    "\n",
    "    h = Conv2DTranspose(\n",
    "        output_channels=int(self._num_hiddens / 2),\n",
    "        output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_2\")(h)\n",
    "    h = tf.nn.relu(h)\n",
    "\n",
    "    x_recon = Conv2DTranspose(\n",
    "        output_channels=3,\n",
    "        output_shape=None,\n",
    "        kernel_shape=(4, 4),\n",
    "        stride=(2, 2),\n",
    "        name=\"dec_3\")(h)\n",
    "\n",
    "    return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonnet as snt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "image_size = 32\n",
    "\n",
    "num_training_updates = 50000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "\n",
    "num_embeddings = 512\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "vq_use_ema = False\n",
    "\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_iterator = (\n",
    "    tf.data.Dataset.from_tensor_slices(anime_train_ds)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)\n",
    "    .batch(batch_size)).make_one_shot_iterator()\n",
    "\n",
    "train_dataset_batch = train_dataset_iterator.get_next()\n",
    "\n",
    "\n",
    "valid_dataset_iterator = (\n",
    "    tf.data.Dataset.from_tensor_slices(anime_val_ds)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)\n",
    "    .batch(batch_size)).make_initializable_iterator()\n",
    "\n",
    "train_dataset_batch = train_dataset_iterator.get_next()\n",
    "valid_dataset_batch = valid_dataset_iterator.get_next()\n",
    "\n",
    "valid_dataset_batch = valid_dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "pre_vq_conv1 = Conv2D(\n",
    "    output_channels=embedding_dim, kernel_shape=(1, 1), stride=(1, 1), name=\"to_vq\"\n",
    ")\n",
    "\n",
    "vq_vae = snt.nets.VectorQuantizer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_embeddings=num_embeddings,\n",
    "    commitment_cost=commitment_cost,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, image_size, image_size, 3))\n",
    "z = pre_vq_conv1(encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_output_train = vq_vae(z, is_training=True)\n",
    "x_recon = decoder(vq_output_train[\"quantize\"])\n",
    "recon_error = tf.reduce_mean((x_recon - x) ** 2)\n",
    "loss = recon_error + vq_output_train[\"loss\"]\n",
    "\n",
    "vq_output_eval = vq_vae(z, is_training=False)\n",
    "x_recon_eval = decoder(vq_output_eval[\"quantize\"])\n",
    "\n",
    "perplexity = vq_output_train[\"perplexity\"]\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "sess = tf.train.SingularMonitoredSession()\n",
    "\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    feed_dict = {x: sess.run(train_dataset_batch)}\n",
    "    results = sess.run([train_op, recon_error, perplexity], feed_dict)\n",
    "    train_res_recon_error.append(results[1])\n",
    "    train_res_perplexity.append(results[2])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"%d iterations\" % i)\n",
    "        print(\"recon_error: %.3f\" % np.mean(train_res_recon_error[-100:]))\n",
    "        print(\"perplexity: %.3f\" % np.mean(train_res_perplexity[-100:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16, 8))\n",
    "ax = f.add_subplot(1, 2, 1)\n",
    "ax.plot(train_res_recon_error)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"NMSE.\")\n",
    "\n",
    "ax = f.add_subplot(1, 2, 2)\n",
    "ax.plot(train_res_perplexity)\n",
    "ax.set_title(\"Average codebook usage (perplexity).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
